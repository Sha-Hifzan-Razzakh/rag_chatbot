from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List, Literal, Optional

from pydantic import BaseModel, Field

from app.audio.types import TTSResponseFormat


# --- Shared types ---


# Expanded for agentic mode (keeps old values valid).
RoleLiteral = Literal["system", "user", "assistant", "tool"]
IntentLiteral = Literal["RAG_QA", "CHITCHAT", "OTHER"]


class Message(BaseModel):
    """
    Single message in the chat history.

    Backwards compatible with your existing usage (role+content).
    Expanded for agentic tool messages (optional fields).
    """

    role: RoleLiteral = Field(
        ...,
        description='Message role, e.g. "system", "user", "assistant", or "tool".',
    )
    content: Optional[str] = Field(
        ...,
        description="Plain text content of the message. Tool/system messages may be empty when using structured payloads.",
    )

    # Optional fields for tool/function calling flows
    name: Optional[str] = Field(
        default=None,
        description="Optional name (e.g. tool name for tool messages, or assistant name).",
    )
    tool_call_id: Optional[str] = Field(
        default=None,
        description="If this message is a tool result, the tool_call_id it corresponds to.",
    )
    tool_name: Optional[str] = Field(
        default=None,
        description="Tool name producing this tool message (if role=tool).",
    )
    created_at: Optional[datetime] = Field(
        default=None,
        description="Optional timestamp for the message.",
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional metadata for the message (debug, internal tags, etc.).",
    )


# Agentic alias (optional; handy to keep naming consistent)
ChatMessage = Message


class ToolCall(BaseModel):
    """
    A single tool/function call request (from the LLM or generated by the orchestrator).
    """

    id: str = Field(..., description="Unique tool call identifier.")
    name: str = Field(..., description="Tool/function name to execute.")
    arguments: Dict[str, Any] = Field(
        default_factory=dict,
        description="Parsed JSON arguments to pass into the tool.",
    )


class ToolResult(BaseModel):
    """
    Result of executing a tool/function call.
    """

    tool_call_id: str = Field(..., description="Tool call id this result corresponds to.")
    name: str = Field(..., description="Tool/function name that was executed.")
    output: Any = Field(
        default=None,
        description="Tool output payload (dict/str/etc.).",
    )
    error: Optional[str] = Field(
        default=None,
        description="Error message if the tool failed.",
    )
    latency_ms: Optional[float] = Field(
        default=None,
        description="Tool execution time in milliseconds.",
    )


class AgentTraceStep(BaseModel):
    """
    One step in the agent loop trace (only returned when debug=True).
    """

    step: int = Field(..., description="Step counter (keep consistent: 0-based or 1-based).")
    type: Literal["llm", "tool", "stop", "error"] = Field(
        ...,
        description="Step type classification.",
    )

    input: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Sanitized input sent to the LLM/tool. Keep small.",
    )
    output: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Sanitized output from the LLM/tool. Keep small.",
    )

    tool_call: Optional[ToolCall] = Field(
        default=None,
        description="Tool call details for tool steps.",
    )
    tool_result: Optional[ToolResult] = Field(
        default=None,
        description="Tool result details for tool steps.",
    )

    tokens_in: Optional[int] = Field(default=None, description="LLM prompt tokens for this step (if available).")
    tokens_out: Optional[int] = Field(default=None, description="LLM completion tokens for this step (if available).")
    latency_ms: Optional[float] = Field(default=None, description="Step latency in milliseconds.")
    timestamp: Optional[datetime] = Field(default=None, description="Optional timestamp for the step.")


class AgentStopReason(BaseModel):
    """
    Why the agent loop stopped.
    """

    reason: Literal[
        "completed",
        "max_steps",
        "max_tool_calls",
        "blocked_tool",
        "llm_error",
        "tool_error",
        "cancelled",
    ] = Field(..., description="Stop reason.")
    detail: Optional[str] = Field(default=None, description="Optional human-readable detail.")


class AgentResult(BaseModel):
    """
    Output of the agent orchestrator.
    """

    conversation_id: str = Field(..., description="Conversation id for the session.")
    message: ChatMessage = Field(..., description="Final assistant message.")
    stop: AgentStopReason = Field(..., description="Stop reason for the agent loop.")

    trace: Optional[List[AgentTraceStep]] = Field(
        default=None,
        description="Agent trace steps (only present when debug enabled).",
    )
    tool_calls: Optional[List[ToolCall]] = Field(
        default=None,
        description="Flattened list of tool calls made (optional convenience).",
    )
    usage: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Aggregate usage stats (tokens, cost, etc.) if available.",
    )


# --- Chat endpoint models ---


class ChatRequest(BaseModel):
    """
    Request body for /v1/chat endpoint.
    """

    # NEW (agentic)
    conversation_id: Optional[str] = Field(
        default=None,
        description="Optional conversation id. If omitted, the server may create one.",
    )
    debug: bool = Field(
        default=False,
        description="If true, include agent trace data in the response (for debugging).",
    )

    question: str = Field(
        ...,
        description="The current user question or message.",
    )
    history: List[Message] = Field(
        default_factory=list,
        description="Previous messages in the conversation (user + assistant).",
    )
    tone: Optional[str] = Field(
        default="neutral",
        description='Optional tone hint, e.g. "neutral", "formal", "casual".',
    )
    style: Optional[str] = Field(
        default="concise",
        description='Optional style hint, e.g. "concise", "detailed", "bullet_points".',
    )
    top_k: Optional[int] = Field(
        default=None,
        description="Number of context documents to retrieve; falls back to DEFAULT_TOP_K.",
        ge=1,
    )
    namespace: Optional[str] = Field(
        default=None,
        description="Optional namespace / knowledge base identifier for multi-tenant setups.",
    )
    return_audio: bool = Field(
        default=False,
        description=(
            "If true, the backend will also generate TTS audio for the assistant's answer and return it "
            "(typically as base64) in the chat response."
        ),
    )


class Source(BaseModel):
    """
    Single retrieved source snippet used to answer a question.
    """

    id: str = Field(
        ...,
        description="Identifier of the source (e.g. document ID or chunk ID).",
    )
    title: Optional[str] = Field(
        default=None,
        description="Human-readable title of the source document, if available.",
    )
    snippet: Optional[str] = Field(
        default=None,
        description="Short snippet of the source text relevant to the answer.",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Arbitrary metadata (file path, page number, etc.).",
    )


class ChatResponse(BaseModel):
    """
    Response body for /v1/chat endpoint.
    """

    # NEW (agentic)
    conversation_id: str = Field(
        ...,
        description="Conversation id for the session.",
    )
    trace: Optional[List[AgentTraceStep]] = Field(
        default=None,
        description="Agent execution trace (only present when debug=true).",
    )

    answer: str = Field(
        ...,
        description="Final answer text returned by the assistant.",
    )
    sources: List[Source] = Field(
        default_factory=list,
        description="List of source snippets that were used for this answer.",
    )
    suggested_questions: List[str] = Field(
        default_factory=list,
        description="Suggested follow-up questions for better exploration.",
    )
    intent: IntentLiteral = Field(
        ...,
        description='Detected query intent, e.g. "RAG_QA", "CHITCHAT", or "OTHER".',
    )
    answer_audio_b64: str | None = Field(
        default=None,
        description="If return_audio=true and TTS succeeds, base64 audio payload.",
    )


# --- Ingestion endpoint models ---


class IngestTextRequest(BaseModel):
    """
    Request body for /v1/ingest/text endpoint.
    """

    texts: List[str] = Field(
        ...,
        description="Raw text documents to ingest into the vector store.",
        min_length=1,
    )
    namespace: Optional[str] = Field(
        default=None,
        description="Optional namespace / knowledge base identifier.",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Metadata applied to all ingested chunks from these texts.",
    )


class STTResponse(BaseModel):
    text: str
    language: Optional[str] = None


class TTSRequest(BaseModel):
    text: str
    voice: Optional[str] = None
    format: Optional[TTSResponseFormat] = None


class IngestResponse(BaseModel):
    """
    Generic ingestion response (kept for backward compatibility).
    """

    status: str = Field(..., description='Status string, usually "ok" on success.')
    num_documents: int = Field(..., description="Number of documents ingested.")
    num_chunks: int = Field(..., description="Total number of chunks stored in the vector DB.")
    namespace: Optional[str] = Field(default=None, description="Namespace used for ingestion, if any.")


class FileIngestResult(BaseModel):
    """
    Per-file ingestion result (for /ingest/file(s)).
    """

    filename: str = Field(..., description="Original uploaded filename.")
    content_type: Optional[str] = Field(default=None, description="MIME type from client, if provided.")
    size_bytes: Optional[int] = Field(default=None, description="Uploaded file size in bytes, if known.")

    status: Literal["ok", "skipped", "error"] = Field(..., description="Result status for this file.")
    reason: Optional[str] = Field(
        default=None,
        description="If skipped/error, a human-readable reason.",
    )

    num_chunks: int = Field(default=0, description="Chunks generated from this file.")
    char_count: Optional[int] = Field(default=None, description="Extracted character count (post-preprocess).")

    # Useful for PDFs
    num_pages: Optional[int] = Field(default=None, description="Number of pages (PDF), if available.")
    ocr_used: Optional[bool] = Field(default=None, description="Whether OCR was used (PDF), if applicable.")


class IngestFilesResponse(BaseModel):
    """
    Rich response for file ingestion.
    """

    status: str = Field(..., description='Overall status string, usually "ok".')
    num_documents: int = Field(..., description="How many files were ingested successfully.")
    num_chunks: int = Field(..., description="Total chunks stored for this request.")
    namespace: Optional[str] = Field(default=None, description="Namespace used for ingestion, if any.")

    results: List[FileIngestResult] = Field(
        default_factory=list,
        description="Per-file results (ok/skipped/error).",
    )
